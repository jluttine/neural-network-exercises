\documentclass[10pt]{article}
\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{subfigure}
\usepackage{epsfig}
\usepackage{amsmath,amssymb}
\parindent 0mm
\textwidth 16cm
\textheight 23cm
\oddsidemargin 0cm
\evensidemargin 0cm
\topmargin -10mm
\newcommand{\vect}[1]{{\bf{#1}}}
\newcommand{\svect}[1]{\boldsymbol{#1}}
\newcommand{\matr}[1]{\boldsymbol{#1}}


\begin{document}
\pagestyle{empty}
\begin{Large}
\begin{bf} 
T-61.5130 Machine Learning and Neural Networks\\ 
\end{bf}
\end{Large}
Karhunen, Hao Tele\\  
\\
\begin{large}
\begin{bf}
Exercise 7,  25.11.2011
\end{bf}
\end{large}
\begin{enumerate}

\item Consider on a general level the major differences (and similarities)
  between radial-basis function (RBF) networks and multilayer perceptron
  (MLP) networks.

\vspace{2mm}

\item The well-known XOR (Exclusive OR) problem is the simplest example of a
  two-class classification problem where the pattern vectors are not linearly
  separable. In the XOR problem, there are four two-dimensional input vectors
  (patterns) (0,0), (0,1), (1,1), and (1,0). The first and third pattern
  vector belong to the class 1, while the second and the fourth one belong
  to the class 2. Solve the XOR problem by using a RBF network with two
  Gaussian basis functions centered at ${\bf c}_1 = (0,1)^T$ and
  ${\bf c}_2 = (1,0)^T$.

\vspace{2mm}

\item Derive the learning rules for the stochastic gradient method for
  training an RBF neural network.

\vspace{2mm}

\item Consider approximation of the nonlinear function $y = e^{-x} \sin(3x)$
  on the interval $[0,4]$ using multilayer perceptron (MLP) and radial-basis
  function (RBF) networks. Compare the generalization ability of these
  networks.

\vspace{2mm}

\item In Ham's and Kostanic's book, training of RBF networks is considered for the
  case of a single neuron in the output layer only. Show how the learning method
  presented in section 3.6.1 can be generalized for several neurons in the
  output layer.


\end{enumerate}
\end{document}             % End of document.
