\documentclass[10pt]{article}
\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{subfigure}
\usepackage{epsfig}
\usepackage{amsmath,amssymb}
\parindent 0mm
\textwidth 16cm
\textheight 23cm
\oddsidemargin 0cm
\evensidemargin 0cm
\topmargin -10mm
\newcommand{\vect}[1]{{\bf{#1}}}
\newcommand{\svect}[1]{\boldsymbol{#1}}
\newcommand{\matr}[1]{\boldsymbol{#1}}


\begin{document}
\pagestyle{empty}
\begin{Large}
\begin{bf} 
T-61.5130 Machine Learning and Neural Networks\\ 
\end{bf}
\end{Large}
Karhunen, Hao Tele\\  
\\
\begin{large}
\begin{bf}
Exercise 3,  11.11.2011
\end{bf}
\end{large}
\begin{enumerate}

\item Consider a wide-sense-stationary
  first-order discrete-time Markov process given by the stochastic
  difference equation
\[
{\bf x}(n) = \alpha {\bf x}(n-1) + {\bf v}(n)
\]
where $\alpha = 0.9; {\bf x}, {\bf v} \in \mathbb{R}^{3\times 1}$,
and ${\bf v}$ is zero-mean Gaussian white noise with a covariance
matrix given as
\begin{equation*}
\textbf{C}_v = \left[ \begin{array}{ccc}
5 & 0 & 0 \\
0 & 3 & 0 \\
0 & 0 & 0.1 \end{array} \right]
\end{equation*}

a) Calculate the covariance matrix ${\bf C}_x = \text{E}\left[ {\bf
    xx}^T \right]$\\
b) Calculate the theoretical eigenvalues and eigenvectors of ${\bf C}_x$

\vspace{2mm}

\item  a) Modify the standard Gram-Schmidt orthogonalization (GSO) method for
performing whitening. b) What can you say about the uniqueness of whitening in view of the
developed algorithm?


\vspace{2mm}

\item a) Develop a whitening method based on principal component analysis.\\
b) Show that if the whitening matrix is multiplied by any orthogonal
matrix, the whitening property remains still valid.


\vspace{2mm}

\item Consider the problem of maximizing the variance of $y_m = {\bf
  w}_m^T{\bf x}\; (m=1, ..., n)$ under the constraint that ${\bf w}_m$
  must be of unit Euclidian norm and orthogonal to all the
  previously-found principal vectors ${\bf w}_i$, $i<m$. Show that the solution
  is given by ${\bf w}_m={\bf e}_m$ the eigenvector of ${\bf C}_x$ corresponding to the mth
  largest eigenvalue.

\vspace{2mm}

\item The learning rule for a PCA neuron is based on maximization of
  $y=({\bf w}^T{\bf x})^2$ under constraint $ \| {\bf w} \| = 1$. (We have now omitted the
  subscript 1 because only one neuron is involved.)

Show that an unlimited gradient ascent method would compute the new
vector ${\bf w}$ from
\[
{\bf w} \leftarrow {\bf w} + \gamma ({\bf w}^T{\bf x}){\bf x}
\]
with $\gamma$ the learning rate. Show that the norm of the weight
vector always grows in this case.



\end{enumerate}
\end{document}             % End of document.
