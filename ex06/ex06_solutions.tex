\documentclass[10pt]{article}
\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{subfigure}
\usepackage{epsfig}
\usepackage{amsmath,amssymb}
\parindent 0mm
\textwidth 16cm
\textheight 23cm
\oddsidemargin 0cm
\evensidemargin 0cm
\topmargin -10mm
\newcommand{\vect}[1]{{\bf{#1}}}
\newcommand{\svect}[1]{\boldsymbol{#1}}
\newcommand{\matr}[1]{\boldsymbol{#1}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\x}{{\bf x}}



\begin{document}
\pagestyle{empty}
\begin{Large}
\begin{bf} 
T-61.5130 Machine Learning and Neural Networks\\ 
\end{bf}
\end{Large}
Karhunen, Hao Tele\\  
\\
\begin{large}
\begin{bf}
Exercise 6,  24.11.2011\\Model answer
\end{bf}
\end{large}
\begin{enumerate}

\item One of the important matters to consider in the design of an MLP
  network is its capability to generalize. Generalization means that
  the network does not give a good response only to the training
  samples but also to new samples from the same distribution. Good 
  generalization can be
  obtained only if the number of the free parameters of the network
  is kept reasonable. As a thumb rule, the number of training samples
  should be at least five times the number of parameters. If there are
  less training samples than parameters, the network easily overlearns
  -- it handles perfectly the training samples but gives arbitrary
  responses to all the other samples.

  An MLP is used for a classification task in which the samples are
  divided into five classes. The input vectors of the network consist
  of ten features and the size of the training set is 800. How many
  hidden units can there be at most according to the rule given above?

---------------------------------------------------------------------------------------------

Let $x$ be the number of hidden neurons. Each hidden neuron has
10 weights for the inputs and one bias term. Each output neuron has one
weight for each hidden neuron and one bias term. Thus the total number
of weights in the network is:
\begin{equation*}
N=\underbrace{(10+1)x}_{\text{input
    layer}}+\underbrace{(x+1)5}_{\text{output layer}}=16x+5
\end{equation*}
Thumb rule:\\
The number of  training samples should be at least five times the
number of parameters.
\begin{equation*}
5N\leq800 \Rightarrow 5(16x+5)\leq 800\Rightarrow 80x+25\leq
800\Rightarrow x\leq9.6875
\end{equation*}
A reasonable number of hidden units is less than 10.
\vspace{1cm}


\vspace{2mm}

\vspace{2cm}
\item Consider several neural networks each of which is considered to
  be the expert in a committee of neural networks (experts).
  What can you say about the bias and variance of a committee in
  which the output is computed with ensemble averaging? Hint 1: Any
  answer is better than none at all. 
  %Hint 2: the output $y$ is
  %obtained like in the previous problem but with equal weights for
  %each expert, $w_k=1/K$.

---------------------------------------------------------------------------------------------

Averaging over $K$ experts does not change the bias: for a given
input $\x$, we have
\begin{displaymath}
(f(\x)-E_{\T}[y])^2 = \left(f(\x)-E_{\T}\left[\sum_{k=1}^K \frac{F_k(\x)}{K}\right]\right)^2
= \left(f(\x)-\sum_{k=1}^K \frac{E_{\T}\left[F(\x)\right]}{K}\right)^2
\end{displaymath}
\begin{displaymath}
= (f(\x)-E_{\T}[F(\x)])^2
\end{displaymath}
  where $f(\x)$ denotes the desired function value and $\T$ denotes the training set;
  the second equality follows because the experts are identical, and thus have an
  identical expected output, here denoted $E_\T[F(\x)]$.

  The variance decreases by factor $1/K$ if the learning of different
  experts is independent  because variances of independent variables
  add up and multiplying with $w_k = 1/K$ decreases the variance by
  factor $1/K^2$. In more detail, we have:
\begin{displaymath}
E_{\T}[(y-E_{\T}[y])^2] = E_\T\left[\left(\sum_{k=1}^K F_k(\x)/K - E_\T [\sum_{k=1}^K F_k(\x)/K]\right)^2\right]
\end{displaymath}
\begin{displaymath}
= E_\T\left[\left(\sum_{k=1}^K (v_k(\x))/K\right)^2\right]
= E_\T\left[\sum_{k,l=1}^K v_k(\x) v_l(\x)\right]/K^2
\end{displaymath}
\begin{displaymath}
= \sum_{k=1}^K E_\T[ v_k^2(\x)]/K^2 + \sum_{k,l=1, k\neq l}^K E_\T[v_k(\x)] E_\T[v_l(\x)]/K^2
= \sum_{k=1}^K E_\T[v^2(\x)]/K^2 = E_\T[v^2(\x)]/K
\end{displaymath}
where $v_k(\x)=F_k(\x)-E_\T[F_k(\x)]$; with this notation,
$E_\T[v_k^2(\x)]$ is the variance of expert $k$. If the learning of
the different experts is independent, the $v_k$ are independent random
variables, which yields the fourth equality. The fifth follows because
$E_\T[v_k(\x)]=E_\T[F_k(\x)]-E_\T[F_k(\x)]=0$, and because identical
experts have identical variances, here denoted $E_\T[v^2(\x)]$.



\vspace{2mm}

\vspace{2cm}
\item Consider the general linear model for modeling a scalar variable \[y:
  y({\bf x},{\bf w}) = \sum_{j=0}^{M-1} w_j \phi_j({\bf x})\]
  where ${\bf x}$ is data vector, ${\bf w}$ is $M$-dimensional weight vector with
  elements $w_0,w_1,...,w_{M-1}$, and the $\phi_j({\bf x})$, $j=1,...,M-1$ are
  some basis functions which can be nonlinear. Often $\phi_0({\bf x})
  = 1$ is the dummy 'basis function' corresponding to the bias term $w_0$.
  You have at your disposal $N$ input-output pairs ($t_i,{\bf x}_i$). Model
  the dependence between input vector ${\bf x}$ and scalar output t using the
  general linear model above. Fit the model to the training data using
  the least-squares method with the added weight decay regularizer
  $0.5\lambda {\bf w}^T{\bf w}$.

---------------------------------------------------------------------------------------------

Least-mean-square \& weight decay

\[
y(\vect{x},\vect{w})=\sum_{j=0}^{M-1} w_j \phi_j(\vect{x}) = \vect{w}^T \phi(\vect{x}) = \phi^T(\vect{x})\vect{w}
\]
where $\phi^T(\vect{x}) = \left[ \phi_0(\vect{x}), ..., \phi_{M-1}(\vect{x})  \right]^T$

Cost function (LMS)
\[
\mathcal{E}_D(\vect{w}) = \frac{1}{2}\sum_{n=1}^{N} \lbrace t_n -
  y(\vect{x}_n,\vect{w}) \rbrace^2
=\frac{1}{2}\sum_{n=1}^{N} \lbrace t_n - \phi^T(\vect{x}_n)\vect{w} \rbrace^2
=\frac{1}{2} (\vect{t}-\Phi \vect{w})^T(\vect{t}-\Phi \vect{w})
\]
where the observation matrix $\Phi$ is
\begin{equation*}
\Phi =
\left[
\begin{array}{cccc}
\phi_0(\vect{x}_1) & \phi_1(\vect{x}_1) & \cdots & \phi_{M-1}(\vect{x}_1) \\
\vdots             & \vdots             & \ddots & \vdots  \\
\phi_0(\vect{x}_N) & \phi_1(\vect{x}_N) & \cdots & \phi_{M-1}(\vect{x}_N) \\
\end{array}
\right]
\end{equation*}

Cost function, including weight decay:
\[
\begin{split}
\mathcal{E}_D(\vect{w}) &= \frac{1}{2} (\vect{t}-\Phi \vect{w})^T(\vect{t}-\Phi
\vect{w}) + \frac{\lambda}{2} \vect{w}^T\vect{w}\\
&=\frac{1}{2}\vect{t}^T\vect{t} - \vect{t}^T\Phi \vect{w} + \frac{1}{2}\vect{w}^T\Phi^T \Phi
\vect{w} +\frac{\lambda}{2}\vect{w}^T\vect{w}
\end{split}
\]

\[
\frac{\partial \mathcal{E}}{\partial \vect{w}} = \Phi^T \vect{t} + \Phi^T
\Phi \vect{w} + \lambda \vect{w} = 0
\]

\[
\Rightarrow (\Phi^T \Phi + \lambda \matr{I})\vect{w} = \Phi^T \vect{t}
\]

\[
\vect{w}_{\text{opt}} = (\Phi^T \Phi + \lambda \matr{I})^{-1}\Phi^T \vect{t}
\]


\vspace{2mm}

\vspace{2cm}
\item Assume that the relationship between the input vector ${\bf x}$
  and the desired response (output) vector ${\bf d}$ is of the form
  \[{\bf d} = {\bf h}({\bf x}) + {\bf e}\]
  where ${\bf h}({\bf x})$ is the true mapping between ${\bf x}$ and ${\bf d}$
  and ${\bf e}$ is the error or noise vector. Consider modeling the unknown
  true mapping ${\bf h}({\bf x})$ by the output ${\bf y}({\bf x},{\bf w})$
  of a neural network, where the vector ${\bf w}$ contains all the adjustable
  weights of the neural network. Assume that you have at your disposal
  $N$ training pairs $({\bf x}_i,{\bf d}_i)$ of the mapping. Show that if the
  training pairs are independent, and the noise vector ${\bf e}$
  is Gaussian with zero mean and covariance matrix $\sigma^2 {\bf I}$, the standard
  least-squares method and maximum likelihood method provide the same results.

---------------------------------------------------------------------------------------------

Connection between Maximum Likelihood and Sum-of-Squares Criteria:

Denote again by ${\bf x}$ the input vector and by ${\bf d}$ the
corresponding desired response (output) vector. The most complete description of the relationship between
${\bf x}$ and ${\bf d}$ is given by their joint probability density
$p({\bf x},{\bf d})$.
This can be written
\[
p({\bf x},{\bf d}) = p({\bf d} \mid {\bf x}) p({\bf x})
\]
In predicting ${\bf d}$ for new values of ${\bf x}$, we need to
model only the conditional density $p({\bf d} \mid {\bf x})$.


A feedforward network can be regarded as a framework for modeling
the conditional density $p({\bf d} \mid {\bf x})$. Note that the density $p({\bf x})$ of the data does not depend
on the network parameters. Suppose that we know $N$ training pairs $({\bf x}_i,{\bf d}_i)$,
$i=1,\ldots,N$ of the input-output mapping. Assuming that these training pairs are statistically independent,
their joint conditional density decouples into
\[
p({\bf d}_1,\ldots,{\bf d}_N \mid {\bf x}_1,\ldots,{\bf x}_N) =
\prod_{i=1}^N p({\bf d}_i \mid {\bf x}_i)
\]
The maximum likelihood method tries to maximize this density by optimizing the weights.
However, the values maximizing the above joint conditional density maximize
its logarithm, too. Hence, one can seek for the values which minimize its
negative logarithm
\[
{\cal L} = - \sum_{i=1}^N \ln p({\bf d}_i \mid {\bf x}_i)
\]

Assume generally that the relationship between the desired vector
${\bf d}$ and input vector ${\bf x}$ is given by
\[
{\bf d} = {\bf h}({\bf x}) + {\bf e}
\]
where ${\bf h}$ is the true mapping between ${\bf x}$ and ${\bf d}$
and ${\bf e}$ is the error or noise vector.


Assume further that the additive noise vector ${\bf e}$ is
{\em Gaussian} with zero mean and covariance matrix $\sigma^2 {\bf I}$.
That is, the components of the noise vector ${\bf e}$ have zero mean,
{\em equal variance} $\sigma^2$, and they are {\em mutually uncorrelated}.
The probability density of ${\bf e}$ is thus
\[
p({\bf e}) = K \exp \left( -\frac{ \parallel {\bf e} \parallel^2 }{2\sigma^2}
\right) = K \exp \left( -\frac{ \parallel {\bf d} - {\bf h}({\bf x})
\parallel^2}{2\sigma^2} \right)
\]
The scaling constant $K$ = $(2\pi \sigma^2)^{-m/2}$, where $m$ is
the dimension of the desired vector ${\bf d}$. A neural network tries to model the unknown true input-output
mapping ${\bf h}({\bf x})$ by its output ${\bf y}({\bf x};{\bf w})$.
The vector ${\bf w}$ contains here all the adjustable weights of the
network.
Replacing ${\bf h}({\bf x})$ by ${\bf y}({\bf x};{\bf w})$ yields
\[
p({\bf d} \mid {\bf x}) = K \exp \left( -\frac{ \parallel {\bf d}
-{\bf y}({\bf x};{\bf w}) \parallel^2 }{2\sigma^2} \right)
\]

Inserting this into the criterion ${\cal L}$ gives
\[
{\cal L} = \frac{1}{2\sigma^2} \sum_{i=1}^N \parallel {\bf d}_i
-{\bf y}({\bf x}_i;{\bf w}) \parallel^2 + N \ln K
\]
Since $\sigma^2$ and $N \ln K$ are constants, ${\cal L}$ is minimized
when the sum-of-squares
\[
{\cal E} = \sum_{i=1}^N \parallel {\bf d}_i -{\bf y}({\bf x}_i;{\bf w}) \parallel^2
\]
is minimized.



\end{enumerate}
\end{document}             % End of document.
