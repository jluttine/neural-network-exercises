\title{T-61.5130 Machine Learning and Neural Networks}
\author{Karhunen, Luttinen}
\date{Exercise 6}

\newcommand{\vect}[1]{{\bf{#1}}}
\newcommand{\svect}[1]{\boldsymbol{#1}}
\newcommand{\matr}[1]{\boldsymbol{#1}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\x}{{\bf x}}



\begin{document}

\maketitle

\begin{enumerate}

\item One of the important matters to consider in the design of an MLP
  network is its capability to generalize. Generalization means that
  the network does not give a good response only to the training
  samples but also to new samples from the same distribution. Good
  generalization can be obtained only if the number of the free
  parameters of the network is kept reasonable. As a thumb rule, the
  number of training samples should be at least five times the number
  of parameters. If there are less training samples than parameters,
  the network easily overlearns -- it handles perfectly the training
  samples but gives arbitrary responses to all the other samples.

  An MLP is used for a classification task in which the samples are
  divided into five classes. The input vectors of the network consist
  of ten features and the size of the training set is 800. How many
  hidden units can there be at most according to the rule given above?

  \begin{solution}

    Let $x$ be the number of hidden neurons. Each hidden neuron has
    10 weights for the inputs and one bias term. Each output neuron has one
    weight for each hidden neuron and one bias term. Thus the total number
    of weights in the network is:
    \begin{equation*}
      N=\underbrace{(10+1)x}_{\text{input
          layer}}+\underbrace{(x+1)5}_{\text{output layer}}=16x+5
    \end{equation*}
    Thumb rule:\\
    The number of  training samples should be at least five times the
    number of parameters.
    \begin{equation*}
      5N\leq800 \Rightarrow 5(16x+5)\leq 800\Rightarrow 80x+25\leq
      800\Rightarrow x\leq9.6875
    \end{equation*}
    A reasonable number of hidden units is less than 10.
  \end{solution}
  

\item Consider the general linear model for modeling a scalar
  variable \[y: y({\bf x},{\bf w}) = \sum_{j=0}^{M-1} w_j \phi_j({\bf
    x})\] where ${\bf x}$ is data vector, ${\bf w}$ is $M$-dimensional
  weight vector with elements $w_0,w_1,...,w_{M-1}$, and the
  $\phi_j({\bf x})$, $j=1,...,M-1$ are some basis functions which can
  be nonlinear. Often $\phi_0({\bf x}) = 1$ is the dummy 'basis
  function' corresponding to the bias term $w_0$.  You have at your
  disposal $N$ input-output pairs ($t_i,{\bf x}_i$). Model the
  dependence between input vector ${\bf x}$ and scalar output t using
  the general linear model above. Fit the model to the training data
  using the least-squares method with the added weight decay
  regularizer $0.5\lambda {\bf w}^T{\bf w}$.

  \begin{solution}

    Least-mean-square \& weight decay

    \[
    y(\vect{x},\vect{w})=\sum_{j=0}^{M-1} w_j \phi_j(\vect{x}) = \vect{w}^T \phi(\vect{x}) = \phi^T(\vect{x})\vect{w}
    \]
    where $\phi^T(\vect{x}) = \left[ \phi_0(\vect{x}), ..., \phi_{M-1}(\vect{x})  \right]^T$

    Cost function (LMS)
    \[
    \mathcal{E}_D(\vect{w}) = \frac{1}{2}\sum_{n=1}^{N} \lbrace t_n -
    y(\vect{x}_n,\vect{w}) \rbrace^2
    =\frac{1}{2}\sum_{n=1}^{N} \lbrace t_n - \phi^T(\vect{x}_n)\vect{w} \rbrace^2
    =\frac{1}{2} (\vect{t}-\Phi \vect{w})^T(\vect{t}-\Phi \vect{w})
    \]
    where the observation matrix $\Phi$ is
    \begin{equation*}
      \Phi =
      \left[
        \begin{array}{cccc}
          \phi_0(\vect{x}_1) & \phi_1(\vect{x}_1) & \cdots & \phi_{M-1}(\vect{x}_1) \\
          \vdots             & \vdots             & \ddots & \vdots  \\
          \phi_0(\vect{x}_N) & \phi_1(\vect{x}_N) & \cdots & \phi_{M-1}(\vect{x}_N) \\
        \end{array}
      \right]
    \end{equation*}

    Cost function, including weight decay:
    \[
    \begin{split}
      \mathcal{E}_D(\vect{w}) &= \frac{1}{2} (\vect{t}-\Phi \vect{w})^T(\vect{t}-\Phi
      \vect{w}) + \frac{\lambda}{2} \vect{w}^T\vect{w}\\
      &=\frac{1}{2}\vect{t}^T\vect{t} - \vect{t}^T\Phi \vect{w} + \frac{1}{2}\vect{w}^T\Phi^T \Phi
      \vect{w} +\frac{\lambda}{2}\vect{w}^T\vect{w}
    \end{split}
    \]

    \[
    \frac{\partial \mathcal{E}}{\partial \vect{w}} = -\Phi^T \vect{t} + \Phi^T
    \Phi \vect{w} + \lambda \vect{w} = 0
    \]

    \[
    \Rightarrow (\Phi^T \Phi + \lambda \matr{I})\vect{w} = \Phi^T \vect{t}
    \]

    \[
    \vect{w}_{\text{opt}} = (\Phi^T \Phi + \lambda \matr{I})^{-1}\Phi^T \vect{t}
    \]
  \end{solution}
  

\item Assume that the relationship between the input vector ${\bf x}$
  and the desired response (output) vector ${\bf d}$ is of the form
  \[{\bf d} = {\bf h}({\bf x}) + {\bf e}\]
  where ${\bf h}({\bf x})$ is the true mapping between ${\bf x}$ and ${\bf d}$
  and ${\bf e}$ is the error or noise vector. Consider modeling the unknown
  true mapping ${\bf h}({\bf x})$ by the output ${\bf y}({\bf x},{\bf w})$
  of a neural network, where the vector ${\bf w}$ contains all the adjustable
  weights of the neural network. Assume that you have at your disposal
  $N$ training pairs $({\bf x}_i,{\bf d}_i)$ of the mapping. Show that if the
  training pairs are independent, and the noise vector ${\bf e}$
  is Gaussian with zero mean and covariance matrix $\sigma^2 {\bf I}$, the standard
  least-squares method and maximum likelihood method provide the same results.

  \begin{solution}

    Connection between Maximum Likelihood and Sum-of-Squares Criteria:

    Denote again by ${\bf x}$ the input vector and by ${\bf d}$ the
    corresponding desired response (output) vector. The most complete description of the relationship between
    ${\bf x}$ and ${\bf d}$ is given by their joint probability density
    $p({\bf x},{\bf d})$.
    This can be written
    \[
    p({\bf x},{\bf d}) = p({\bf d} \mid {\bf x}) p({\bf x})
    \]
    In predicting ${\bf d}$ for new values of ${\bf x}$, we need to
    model only the conditional density $p({\bf d} \mid {\bf x})$.


    A feedforward network can be regarded as a framework for modeling
    the conditional density $p({\bf d} \mid {\bf x})$. Note that the density $p({\bf x})$ of the data does not depend
    on the network parameters. Suppose that we know $N$ training pairs $({\bf x}_i,{\bf d}_i)$,
    $i=1,\ldots,N$ of the input-output mapping. Assuming that these training pairs are statistically independent,
    their joint conditional density decouples into
    \[
    p({\bf d}_1,\ldots,{\bf d}_N \mid {\bf x}_1,\ldots,{\bf x}_N) =
    \prod_{i=1}^N p({\bf d}_i \mid {\bf x}_i)
    \]
    The maximum likelihood method tries to maximize this density by optimizing the weights.
    However, the values maximizing the above joint conditional density maximize
    its logarithm, too. Hence, one can seek for the values which minimize its
    negative logarithm
    \[
    {\cal L} = - \sum_{i=1}^N \ln p({\bf d}_i \mid {\bf x}_i)
    \]

    Assume generally that the relationship between the desired vector
    ${\bf d}$ and input vector ${\bf x}$ is given by
    \[
    {\bf d} = {\bf h}({\bf x}) + {\bf e}
    \]
    where ${\bf h}$ is the true mapping between ${\bf x}$ and ${\bf d}$
    and ${\bf e}$ is the error or noise vector.


    Assume further that the additive noise vector ${\bf e}$ is
    {\em Gaussian} with zero mean and covariance matrix $\sigma^2 {\bf I}$.
    That is, the components of the noise vector ${\bf e}$ have zero mean,
    {\em equal variance} $\sigma^2$, and they are {\em mutually uncorrelated}.
    The probability density of ${\bf e}$ is thus
    \[
    p({\bf e}) = K \exp \left( -\frac{ \parallel {\bf e} \parallel^2 }{2\sigma^2}
    \right) = K \exp \left( -\frac{ \parallel {\bf d} - {\bf h}({\bf x})
        \parallel^2}{2\sigma^2} \right)
    \]
    The scaling constant $K$ = $(2\pi \sigma^2)^{-m/2}$, where $m$ is
    the dimension of the desired vector ${\bf d}$. A neural network tries to model the unknown true input-output
    mapping ${\bf h}({\bf x})$ by its output ${\bf y}({\bf x};{\bf w})$.
    The vector ${\bf w}$ contains here all the adjustable weights of the
    network.
    Replacing ${\bf h}({\bf x})$ by ${\bf y}({\bf x};{\bf w})$ yields
    \[
    p({\bf d} \mid {\bf x}) = K \exp \left( -\frac{ \parallel {\bf d}
        -{\bf y}({\bf x};{\bf w}) \parallel^2 }{2\sigma^2} \right)
    \]

    Inserting this into the criterion ${\cal L}$ gives
    \[
    {\cal L} = \frac{1}{2\sigma^2} \sum_{i=1}^N \parallel {\bf d}_i
    -{\bf y}({\bf x}_i;{\bf w}) \parallel^2 + N \ln K
    \]
    Since $\sigma^2$ and $N \ln K$ are constants, ${\cal L}$ is minimized
    when the sum-of-squares
    \[
    {\cal E} = \sum_{i=1}^N \parallel {\bf d}_i -{\bf y}({\bf x}_i;{\bf w}) \parallel^2
    \]
    is minimized.

  \end{solution}
  
\end{enumerate}
\end{document}             % End of document.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "ex06_solutions"
%%% End: 
