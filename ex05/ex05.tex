\documentclass[10pt]{article}
\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{subfigure}
\usepackage{epsfig}
\usepackage{amsmath,amssymb}
\parindent 0mm
\textwidth 16cm
\textheight 23cm
\oddsidemargin 0cm
\evensidemargin 0cm
\topmargin -10mm
\newcommand{\vect}[1]{{\bf{#1}}}
\newcommand{\svect}[1]{\boldsymbol{#1}}
\newcommand{\matr}[1]{\boldsymbol{#1}}


\begin{document}
\pagestyle{empty}
\begin{Large}
\begin{bf} 
T-61.5130 Machine Learning and Neural Networks\\ 
\end{bf}
\end{Large}
Karhunen, Hao Tele\\  
\\
\begin{large}
\begin{bf}
Exercise 5,  18.11.2011
\end{bf}
\end{large}
\begin{enumerate}

\item In the steepest descent method the adjustment
$\Delta\mathbf{w}(n)$ applied to the parameter vector $\mathbf{w}(n)$
is defined by $\Delta\mathbf{w}(n)=-\eta\mathbf{g}(n)$, where $\eta$
is the learning-rate parameter and
\begin{equation*}
\mathbf{g}(n)=\left.\frac{\partial
\mathcal{E}_{av}(\mathbf{w})}{\partial \mathbf{w}}\right|_{\mathbf{w}=\mathbf{w}(n)}
\end{equation*}
is the local gradient vector of cost
function $\mathcal{E}_{av}(\mathbf{w})$ averaged over the learning
samples. How could you determine the learning rate parameter $\eta$ so
that it minimizes the cost function $\mathcal{E}_{av}(\mathbf{w})$ as much as possible?

\vspace{2mm}

\item In Ham's and Kostanic's book, the Levenberg-Marquardt algorithm is derived
  in subsection 3.4.4 as an approximation from Newton's optimization algorithm.
  Derive the Levenberg-Marquardt algorithm in another way by linearizing
  the dependence of the errors $e_p$ on the weight vector ${\bf w}$ using
  Taylor series expansion.

\vspace{2mm}

\item In extreme learning machine, the optimal weights are computed using the
pseudoinverse ${\bf H}^+$ of the hidden layer output matrix ${\bf H}$. Consider
an $N \times M$ matrix ${\bf X}$, which is generally rectangular with $N \neq M$.
The singular value decomposition (SVD) of ${\bf X}$ is a generalization of the
eigendecomposition, defined for square matrices only, to rectangular matrices.
Similarly, the pseudoinverse ${\bf X}^+$ of ${\bf X}$ is a generalization of the
standard inverse matrix defined for square non-singular matrices only to
rectangular matrices. Discuss the definitions, computation, and interrelationships
of the SVD and pseudoinverse of ${\bf X}$.

\vspace{2mm}

\item (Ham \& Kostanic 3.8, Matlab demo) Write a computer program that uses the MLP
  network trained by backpropagation to perform image compression. To
  generate the input/target vectors $\vect{x}$, divide the image into
  8-by-8 pixel blocks and arrange the elements of each block into a
  64-dimensional vector. Experiment with different numbers of neurons
  in the hidden layer.

\end{enumerate}
\end{document}             % End of document.
