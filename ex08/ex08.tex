
\title{T-61.5130 Machine Learning and Neural Networks}
\author{Karhunen, Luttinen}
\date{Exercise 8, ??.??.2012}

\newcommand{\vect}[1]{{\bf{#1}}}
\newcommand{\svect}[1]{\boldsymbol{#1}}
\newcommand{\matr}[1]{\boldsymbol{#1}}

\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\set}[1]{\mathcal{#1}}
\newcommand{\C}{\set{C}}
\newcommand{\E}{\mathcal{E}}
\newcommand{\I}{\vec{I}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\N}{\mathrm{I \negmedspace N}}
\newcommand{\R}{\mathrm{I \negmedspace R}}
\newcommand{\V}{\set{V}}
\newcommand{\W}{\vec{W}}
\newcommand{\X}{\set{X}}
\newcommand{\e}{\vec{e}}
% \newcommand{\f}[1]{\mathrm{#1}} %funktio
\newcommand{\h}{\vec{h}}
\newcommand{\m}{\vec{m}}
\newcommand{\mub}{\boldsymbol{\mu}}
\newcommand{\n}{\vec{n}}
\renewcommand{\t}{\vec{t}}
\renewcommand{\u}{\vec{u}}
\renewcommand{\v}{\vec{v}}
\newcommand{\w}{\vec{w}}
\newcommand{\x}{\vec{x}}
\newcommand{\y}{\vec{y}}
\newcommand{\Y}{\vec{Y}}
\newcommand{\z}{\vec{z}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}


\begin{document}

\maketitle

\begin{enumerate}

\item Starting with the primal problem for the optimization of the
  separating SVM-hyperplane for nonseparable patterns, formulate the
  dual problem for the nonseparable case.

  \begin{solution}

    Now the Lagrangian function is $$J(\w, b, \xi, \alpha, \mu) =
    \frac{1}{2}\w^T\w + C \sum_{i=1}^N \xi_i - \sum_{i=1}^N
    \alpha_i[d_i(\w^T \x_i + b) - 1 + \xi_i] - \sum_{i=1}^N \mu_i \xi_i
    \, ,$$ where $\alpha_i$ and $\mu_i$ are Lagrange multipliers.  The
    new multipliers are due to the constraints $\xi_i \geq 0$.  The
    conditions for optimality are $\partial J/\partial \w = \vec{0}$,
    $\partial J/\partial b = 0$ and $\partial J/\partial \xi = \vec{0}$.
    We thus have $$\frac{\partial J}{\partial \w} = \w - \sum_{i=1}^N
    \alpha_i d_i \x_i = 0 \; \Rightarrow \; \w = \sum_{i=1}^N \alpha_i
    d_i \x_i \, ,$$ $$\frac{\partial J}{\partial b} = - \sum_{i=1}^N
    \alpha_i d_i = 0 \,$$ and $$\frac{\partial J}{\partial \xi_i} = C -
    \alpha_i - \mu_i = 0 \; \Rightarrow \; \alpha_i + \mu_i = C \, .$$
    Inserting these to the Lagrangian gives after some manipulation
    $$Q(\alpha) = \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N
    \sum_{j=1}^N \alpha_i \alpha_j d_i d_j \x_i^T \x_j \, ,$$ where
    $Q(\alpha) = J(\w, b, \xi, \alpha, \mu)$ is the objective function
    of the dual problem.

    See also exercise 5 in T-61.5130 (pattern recognition) available in Noppa, which contains more derivations.
  \end{solution}
  
\item In the case of separable classes, what happens to the separating
  hyperplane of SVMs if the amount of noise increases? How about the
  case of nonseparable classes?

  \begin{solution}

    The separating hyperplane is determined by the samples which are
    closest to the wrong class.  With no noise the hyperplane will be
    optimal in the sense that the margin of separation will be as wide
    as possible.  With increasing noise the orientation of the
    hyperplane changes randomly because the positions of the support
    vectors change.  At some point when the noise has increased enough,
    the samples have moved to points which cannot be linearly separated.
    Then the separation can be done using slack-variables which allow
    some points to be on the wrong side of the separating hyperplane.
    Notice that the number of support vectors does not change.
  \end{solution}
  
\item The inner-product kernel for a polynomial learning machine is
  defined by
  \begin{displaymath}
    K(\x,\x_i) = (1 + \x^T \x_i)^p \; ,
  \end{displaymath}
  where $p$ is a positive integer.  What is the dimension of the
  implicit feature space if the dimension of $\x$ is $m$.  Find lower
  and upper bounds if you cannot find the exact answer.

  \begin{solution}

    Consider the terms of the inner-product kernel $K(\x, \y) =
    (1 + x_1 y_1 + \ldots x_m y_m)^p$.  For instance, $m=2$ and $p=2$
    yields $$K(x, y) = (1 + x_1y_1 + x_2y_2)^2 = 1 + 2x_1y_1 + 2x_2y_2 +
    x_1^2y_1^2 + x_2^2y_2^2 + 2x_1x_2y_1y_2 = $$ $$[1 \; \sqrt{2}x_1 \;
    \sqrt{2}x_2 \; x_1^2 \; x_2^2 \; \sqrt{2}x_1 x_2] [1 \; \sqrt{2}y_1
    \; \sqrt{2}y_2 \; y_1^2 \; y_2^2 \; \sqrt{2}y_1 y_2]^T \, .$$
    Therefore the implicit features are $1$, $\sqrt{2}x_1$,
    $\sqrt{2}x_2$, $x_1^2$, $x_2^2$ and $\sqrt{2}x_1 x_2$, i.e., the
    dimension of the implicit feature space is six.

    In general, each term of $K(\x, \y)$ is of the form $$c 1^{p_0}
    x_1^{p_1} y_1^{p_1} \ldots x_m^{p_m} y_m^{p_m} = (\sqrt{c} x_1^{p_1}
    \ldots x_m^{p_m})(\sqrt{c} y_1^{p_1} \ldots y_m^{p_m}) \, ,$$where c
    is an integer and $\sum_{i=0}^m p_i = p$.  This shows that the
    implicit features are of the form $\sqrt{c} x_1^{p_1} \ldots
    x_m^{p_m}$ and the dimension of the implicit feature space is the
    number of these terms, i.e., the number of partitions $p_i$ which
    satisfy $\sum_{i=0}^m p_i = p$.

    In order to evaluate the number of partitions, we shall construct a
    scheme for coding the numbers $p_i$ as a sequence.  For each $i$,
    the sequence contains $p_i$ symbols $a$ and the border between
    different $p_i$ is denoted by delimiter $b$.  Since there are $m+1$
    numbers $p_i$, the sequence has $m$ symbols $b$.  It is easy to see
    that each sequence having $p$ symbols $a$ and $m$ symbols $b$ can be
    interpreted as a unique set of numbers $p_i$ and vice versa.  The
    number of these sequences and therefore also the dimension of the
    implicit feature space is 
    $$\frac{(m+p)!}{m!p!}.$$
  \end{solution}
  

\item Can a support vector machine be used to solve a pattern
  classification task where there are more than two classes?

  \begin{solution}

    Yes.  For each class, construct a SVM which discriminates
    between that class and all the rest.  This way there will be as many
    SVMs as there are classes.  Alternatively, it is possible to use a
    binary classification as the following example shows: for eight
    classes, train one SVM to separate between classes (1,2,3,4) and
    (5,6,7,8), one SVM to separate between (1,2,5,6) and (3,4,7,8) and
    one SVM to separate (1,3,5,7) and (2,4,6,8).  This way there will be
    only about $\log_2 N$ classifiers for $N$ classes, but the task for
    each SVM will be more difficult and consequently the classification
    result can be worse.

  \end{solution}
  

\end{enumerate}
\end{document}             % End of document.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "ex08_solutions"
%%% End: 
